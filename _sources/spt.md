# Revisiting the Power of Prompt for Visual Tuning
- 文章链接：[http://arxiv.org/abs/2402.02382](http://arxiv.org/abs/2402.02382)
- 时间：2024
## 摘要
基于Transformer的大型语言模型（如BERT和GPT）取得了巨大成功，微调（在特定任务数据集上调整预训练模型）是利用这些模型进行下游任务的标准做法。然而，由于模型规模庞大，Transformer微调**运行时间长且内存消耗高**。我们提出了SPT系统，通过引入稀疏性来高效地微调基于Transformer的模型。我们观察到，Transformer的内存消耗主要来自于存储多头注意力（MHA）的注意力权重，而运行时间的大部分则花在前馈网络（FFN）上。因此，我们设计了稀疏MHA模块，仅计算和存储较大的注意力权重以减少内存消耗，以及路由式FFN模块，为每个标记动态激活模型参数的子集以减少计算成本。我们在PyTorch上实现了SPT，并定制了CUDA内核以高效运行稀疏MHA和路由式FFN。具体来说，我们使用产品量化来识别较大的注意力权重，并通过稀疏矩阵乘法来计算注意力，用于稀疏MHA。对于路由式FFN，我们根据激活的模型参数对标记进行分组，以实现高效计算。我们在不同的模型配置上进行了广泛的实验来评估SPT。*结果显示，SPT一致优于优化良好的基线，将峰值内存消耗减少高达50%，并将微调速度提高多达2.2倍。*